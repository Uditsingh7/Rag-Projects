{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyN3YfcF5pf2JVNlsTEvKEcI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Uditsingh7/Rag-Projects/blob/main/Udit_Rag_Project_GitHub_Issues.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Building** a RAG using an open-source LLM, embeddings model, and LangChain."
      ],
      "metadata": {
        "id": "55JKjyPLa2h6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PVINZAudalzO"
      },
      "outputs": [],
      "source": [
        "## install dependencies\n",
        "\n",
        "!pip install -q torch transformers accelerate bitsandbytes sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc --version"
      ],
      "metadata": {
        "id": "ugA_7b8NbeRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-gpu-cu12"
      ],
      "metadata": {
        "id": "JacnXTS1dxW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## q=quiet\n",
        "!pip install -q langchain"
      ],
      "metadata": {
        "id": "b65qKSxBd3yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community"
      ],
      "metadata": {
        "id": "-9c9URw9hgjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q sentence-transformers"
      ],
      "metadata": {
        "id": "BSyAluhAkG-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from getpass import getpass\n",
        "\n",
        "Access_Token = getpass(\"Please enter your GitHub Personal Access Token: \")\n"
      ],
      "metadata": {
        "id": "oImebDcne2lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from os import access\n",
        "##  load all of the issues in the huggingface/peft repo\n",
        "\n",
        "from langchain.document_loaders import GitHubIssuesLoader\n",
        "\n",
        "## By default, pull requests are considered issues as well,\n",
        "##here we chose to exclude them from data with by setting include_prs=False\n",
        "\n",
        "## state=all, means we will load both open and closed isses\n",
        "\n",
        "repo = \"huggingface/peft\"\n",
        "loader = GitHubIssuesLoader(repo=repo, access_token=Access_Token,\n",
        "                            include_prs=False, state=\"all\")"
      ],
      "metadata": {
        "id": "ba38w9gVfXX9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "hZ-fH3VxhcYW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0]"
      ],
      "metadata": {
        "id": "IxGSFdO3hm2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Keeping some overlap between chunks allows us to preserve some semantic context between the chunks.\n",
        "\n",
        "## The recommended splitter for generic text is the RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512,\n",
        "    chunk_overlap=30,\n",
        ")\n",
        "\n",
        "chnked_docs = splitter.split_documents(docs)\n",
        "\n",
        "chnked_docs[0]"
      ],
      "metadata": {
        "id": "bZtpt8PQh9aL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Created embedders and Vectors\n",
        "\n",
        "## To create document chunk embeddings we’ll use theHuggingFaceEmbeddings and the BAAI/bge-base-en-v1.5 embeddings model\n",
        "\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"BAAI/bge-base-en-v1.5\"\n",
        "embeddings = HuggingFaceEmbeddings(model_name=model_name)\n",
        "\n",
        "embeddings"
      ],
      "metadata": {
        "id": "gAkrjlsKipQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## To create the vector database, we’ll use FAISS, a library developed by Facebook AI.\n",
        "##This library offers efficient similarity search and clustering of dense vectors, which is what we need here.\n",
        "##FAISS is currently one of the most used libraries for NN search in massive datasets.\n",
        "\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "db = FAISS.from_documents(chnked_docs, embeddings)\n",
        "\n",
        "db\n"
      ],
      "metadata": {
        "id": "IM6pVP7qjcPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We need a way to return(retrieve) the documents given an unstructured query.\n",
        "##For that, we’ll use the as_retriever method using the db as a backbone\n",
        "\n",
        "## Declare a retriever method with the vector db\n",
        "retriever = db.as_retriever(search_type='similarity', search_kwargs= {\"k\":4})\n",
        "retriever"
      ],
      "metadata": {
        "id": "OZumNCPelIeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## The vector database and retriever are now set up,\n",
        "##next we need to set up the next piece of the chain — the model.\n"
      ],
      "metadata": {
        "id": "cxMS3jUC1hgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Load quantized model\n",
        "##  we chose HuggingFaceH4/zephyr-7b-beta, a small but powerful model.\n",
        "\n",
        "## To make inference faster, will load the quantized version of the model:\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "model\n"
      ],
      "metadata": {
        "id": "Ed2-7mmo2gKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## We have all the pieces we need to set the LLM chain\n",
        "##loaded doc, splitted the doc, chnked it, embedded it, stored it in the vector db,\n",
        "##defined a retriever method, loaded the qantised model, tokeniser\n",
        "\n",
        "\n",
        "## Setup the LLM chain\n",
        "\n",
        "## first create a text generation pipeline  using the loaded model and its tokenizer.\n",
        "\n",
        "from transformers import pipeline\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "text_generation_pipeline = pipeline(\n",
        "    model=model, # The pre-trained language model object itself. This is typically an instance of a Hugging Face 'AutoModelForCausalLM'.\n",
        "    tokenizer=tokenizer, # The tokenizer corresponding to the 'model'. It's essential for converting text to token IDs and vice-versa, which the model understands.\n",
        "    task='text-generation', # Specifies the task for the pipeline. 'text-generation' means the pipeline will generate new text based on a given prompt.\n",
        "    temperature=0.2, # Controls the randomness of the generated text. A value of 0.2 (low) makes the output more deterministic and focused on high-probability tokens, reducing creativity.\n",
        "    do_sample=True, # Enables sampling-based text generation. When True, the model will pick tokens stochastically based on their probabilities, rather than just the most probable one.\n",
        "    repetition_penalty=1.1, # Penalizes the generation of repeated tokens. A value of 1.1 (slightly above 1.0) discourages the model from repeating words or phrases too often, improving coherence.\n",
        "    return_full_text=True, # Determines if the output should include the input prompt along with the generated text. True means the full text (prompt + generation) is returned.\n",
        "    max_new_tokens=512, # The maximum number of new tokens to generate in addition to the input prompt. A value of 512 sets a reasonable limit on the length of the generated output.\n",
        ")\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
        "llm\n"
      ],
      "metadata": {
        "id": "dh8Pzwwo3k-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Next, create a prompt template — this should follow the format of the model, so if you substitute the model checkpoint,\n",
        "##make sure to use the appropriate formatting.\n",
        "\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "## StrOutputParser: take the output from a Language Model (LLM) and ensure it's returned as a plain string.\n",
        "##Basically extration of content data in the form os string\n",
        "\n",
        "prompt_template = \"\"\"\n",
        "<|system|>\n",
        "Answer the question based on your knowledge. Use the following context to help:\n",
        "{context}\n",
        "</s>\n",
        "<|user|>\n",
        "{question}\n",
        "</s>\n",
        "<|assistant|>\n",
        " \"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=prompt_template\n",
        ")\n",
        "\n",
        "## form a llm chain with prompt and llm\n",
        "## | stands for LangChain Expression Language (LCEL).\n",
        "##Its is sed to form a pipe or sequential composition\n",
        "### It means o/p of starting from left will be  automatically passed on to the next as an i/p\n",
        "llm_chain = prompt | llm | StrOutputParser()\n",
        "\n",
        "llm_chain\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "pWEaWRUh5R7f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "rag_chain = {\"context\": retriever, \"question\": RunnablePassthrough()} | llm_chain\n",
        "\n",
        "rag_chain"
      ],
      "metadata": {
        "id": "pGq9SkdE7-dX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"How do you combine multiple adapters?\""
      ],
      "metadata": {
        "id": "IqAVUcg--FRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## First, let’s see what kind of answer we can get with just the model itself, no context added\n",
        "\n",
        "llm_chain.invoke({\"context\":\"\", \"question\":question})"
      ],
      "metadata": {
        "id": "F35J2jjH-RZf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rag_chain.invoke(question)"
      ],
      "metadata": {
        "id": "E0VT_NC5-W7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-TQ47tXx_A4J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}